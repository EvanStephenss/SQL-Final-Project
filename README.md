# SQL-Final-Project

# Project Name
*So You Want a Lichtenstein*

## Project Objective
*The purpose of this project was to demonstrate my capabilities in Python, SQL, AWS RDS, Webscraping, and API manipulation. I decided to make this project personal by impersonating a data analyst working under an auction house, art gallery, museum, or contractural curator. While its a niche role I admit, I belive there to be great opportunity in bringing the cutting edge of data analytics into the art world. This project is my first step towards that goal.*

## What Problem Are You Solving?
* Considering you are a collector of a particular artist, Roy Lichtenstein for instance. It would be useful to be able to cross reference the database of a massive museum art collection with the current records of an art auction history. I am solving the problem of better insights and analysis on this collection of art by aggregating them together.*

## How Are You Solving This Problem?
*Using Joins, Window Functions, aggregate Functions, and CTEs i will be gathering actionable imsights for lichtenstein collectors*

### Key Points from the Job Description Relevant to My Project

1. **Digital Analytics Role and Responsibilities**: The role of Digital Analytics Senior Manager at Sotheby's focuses heavily on analyzing and reporting on the performance of various marketing campaigns using tools like Adobe Analytics and SQL. This parallels Some of the analytical workin this project where I used SQL to analyze art auction data.
  
2. **Use of Data in Decision Making**: The job requires the ability to manipulate large quantities of data to drive business decisions. This is a skill I demonstrated by integrating data from multiple sources (API and web scraping) to provide actionable insights for Lichtenstein collectors and dealers.

3. **Technical Proficiency**: Proficiency in SQL and R is a must for the role at Sotheby's. My project showcased my capability in SQL, along with various Python libraries, AWS RDS, and other technologies, underlining my exemplary technical adaptability and proficiency.

### Project and Job Posting Relation

This directly addresses the kind of challenges and responsibilities described in the Sotheby's job posting. Specifically, with aggregating and analyzing data aligns with the role's focus on utilizing data to inform strategic marketing decisions. By demonstrating my ability to gather and interpret data related to art collections and auction history, I directly showcase the skills needed to contribute effectively to Sotheby’s digital analytics objectives.

### Summary

Overall, I've effectively utilized SQL and Python to address a specific niche in the art world—enhancing the data analytics capabilities surrounding art collection and auction history. This directly aligns with the responsibilities of the Digital Analytics Senior Manager position at Sotheby's, where the emphasis is on using analytical tools to optimize marketing campaigns and make informed business decisions.My project not only demonstrates relevant technical skills and passion for the subject but also shows an innovative approach to merging data analytics within the art sector. 


## Data Source
*My data is retrieved from The Metropolitan Museum of Art Collection API as well as a comprehensive Roy Lichtenstein auction history webscraped from
artsy.net*

## Notebooks
*Link and describe each Jupyter notebook in your repository. For example:*
- [API_ETL_EvanStephens_Final_Version.ipynb](https://github.com/EvanStephenss/SQL-Final-Project/blob/a337512a188d985feba37ee6a3201e25a53d424c/API_ETL_EvanStephens_Final_Version.ipynb): This notebook contains the API ETL Pipline
- [WEB_SCRAPE_ETL_EvanStephens_Final_Version.ipynb](https://github.com/EvanStephenss/SQL-Final-Project/blob/ae973b7ef70a6c9f00703861aad2c71b74c6ea33/WEB_SCRAPE_ETL_EvanStephens_Final_Version.ipynb): This notebook contains the Webscraping ETL Pipeline

## Future Improvements
*Would have liked to include financial history and pricing information but I was unable to scrape this inmformation due to its access being behind a login. I spent many hours working on a solution to this issue using selenium and an automated browsing function that gave the scaper access to the logged in url but needed to abondon this effort due to time constraints.*

