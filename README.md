# SQL-Final-Project

# Project Name
*So You Want a Lichtenstein*

## Project Objective
*The purpose of this project was to demonstrate my capabilities in Python, SQL, AWS RDS, Webscraping, and API manipulation. I decided to make this project personal by impersonating a data analyst working under an auction house, art gallery, museum, or contractural curator. While its a niche role I admit, I belive there to be great opportunity in bringing the cutting edge of data analytics into the art world. This project is my first step towards that goal.*

## What Problem Are You Solving?
* Considering you are a collector of a particular artist, Roy Lichtenstein for instance. It would be useful to be able to cross reference the database of a massive museum art collection with the current records of an art auction history. I am solving the problem of better insights and analysis on this collection of art by aggregating them together.*

## How Are You Solving This Problem?
*Using Joins, Window Functions, aggregate Functions, and CTEs i will be gathering actionable imsights for lichtenstein collectors*

## Job Description
*Include a paragraph explaining the company, the title of the position, and the job description for the job posting you selected.*

## Project and Job Posting Relation
*Explain how the project relates to the job requirements and responsibilities mentioned in the job posting.*

## Data
### Source
*My data is retrieved from The Metropolitan Museum of Art Collection API as well as a comprehensive Roy Lichtenstein auction history webscraped from
artsy.net*

### Characteristics
*Describe the main characteristics of the data, such as volume, variety, velocity, and veracity.*

## Notebooks
*Link and describe each Jupyter notebook in your repository. For example:*
- [API_ETL_EvanStephens_Final_Version.ipynb]: This notebook is used for initial data exploration and visualization.
- [Notebook2.ipynb](link-to-notebook): This notebook focuses on data preprocessing and model training.

## Future Improvements
*Would have liked to include financial history and pricing information but I was unable to scrape this inmformation due to its access being behind a login. I spent many hours working on a solution to this issue using selenium and an automated browsing function that gave the scaper access to the logged in url but needed to abondon this effort due to time constraints.*

